\documentclass[11pt, a4paper, twocolumn]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{titlesec}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage[top=1cm, left=1cm, right=1cm, bottom=2cm]{geometry}

\usepackage{helvet} % Helvetica 'phv'
\usepackage{mathptmx} % Times 'ptm'

\titleformat{\section}
    {\sffamily\bfseries\large} % format
    {\thesection} % label
    {1em} % label separation
    {} % before-code

\titleformat{\subsection}
    {\sffamily\bfseries} % format
    {\thesubsection} % label
    {1em} % label separation
    {} % before-code

\title{\sffamily\bfseries DRAFT\\An Exploration of Optimisation Techniques\\for Vulkan-based Particle Systems}
\author{Robin Wragg}
\date{\today}

\begin{document}

\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Notes}
"3.3 Rendering system requirements and constraints" of "artist-directable real-time rain etc" has good info on texture compression, vertex compression.
"3.6.2 Raindrop Particles Rain"

\section{Literature Review}

literature was pulled from primarily X areas: a, b, and c.

\subsection{Particle System Design}

\citet{Boulianne2007} implemented a biological system simulator using a 3D grid in which each element can hold one or zero particles. Their simulator needed to take into account the spacial locality of particles; a grid facilitates this by removing the need for distance calculation and particle search. Although their implementation did not have real-time rendering in mind, this grid-based approach could be applied to particle-based rendering for situations where the effect requires particles to react to each other based on their proximity. Additionally, ``this system is expected to be suitable for acceleration with parallel customizable hardware,'' \citep{Boulianne2007} meaning this technique would likely benefit from the parallel nature of GPUs.

When producing a simulation of water droplets on a glass pane... \citet{Chen2012}

\subsection{Techniques for Real-time Graphics}

\citet{Crawford2018} note that shader compiler optimisation can make a modest improvement to graphics performance, but it is highly dependent on shader code itself. With their test suite of GFXBench 4.0 GLSL shaders and the LunarGlass/LLVM optimisation framework, they found that shaders can be sped up by as much as 25\% by finding the best combination of compiler flags, but 1-4\% should be expected in general. Common optimisations that shader compilers can perform are dead code elimination, factoring out conditionals, unrolling loops, coalescing multiple vector element assignments into a single swizzled vector assignment, global value numbering causing variable elimination, and simplifying arithmetic by reordering the statements. The study reviewed only source-to-source optimisations; source-to-machine-code optimisations weren't explored. Further speed-ups could be found in that area.

Referring to Vulkan and DirectX 12, \citet{Joseph2016} states ``the central focus of this new generation of APIs is to increase the amount of draw calls possible while decreasing the amount of overhead for the CPU.'' As graphics programmers, we can reinterpret this to indicate that it is critical to reduce the amount of time that the CPU and GPU are required to block each other to communicate, in order to best utilise the hardware.

\subsection{Multi-threaded Software Design}

Blocked threads are significant cause of the reduction of maximum theoretical performance on multi-core systems \citep{Alemany1992}. While some amount of blocking between the CPU and GPU is a challenge to overcome, there exist programming techniques that allow concurrent sections to communicate without blocking each other, such as \emph{exponential backoff}, an algorithm commonly used in network coordination, that slows a process or thread in order to reduce congestion on a shared resource such as an Ethernet node \citep{Goodman2019}, or more applicably for us, a mutex or critical memory. This of course has the downside of one or more threads not operating at their maximum speed but it can have an overall benefit, depending on the bottlenecks of the program. \emph{Optimistic concurrency control} is another class of algorithms for non-blocking concurrency that involves validating a transaction performed on shared data before committing it \citep{Herlihy1993} in an attempt to detect whether data corruption had occurred due to simultaneous writes or reads. Again, this has a downside of requiring extra work per transaction.

(some of these techniques) are very complex; their usefulness must be weighed against the development team's ability to handle the additional complexity, and the cost of time spent implementing it. For this reason, we began implementing the optimisation techniques that appeared to have a good benefit-to-complexity ratio. (make a chart of these ratios?)

After examining the literature, we decided on a [something] implementation based on the techniques of cite, cite, cite, and cite.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{agsm}
\bibliography{references}
\end{document}





